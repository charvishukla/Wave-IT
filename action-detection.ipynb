{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e966c67b-eecd-46a0-bc97-24a7df955010",
   "metadata": {},
   "source": [
    "### Key Points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9b2e8f9-5cee-4669-98ff-4c609f7eb053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe==0.10.9 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.10.9)\n",
      "Requirement already satisfied: opencv-python in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.8.1.78)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.7.2)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: tensorflow==2.13.0rc2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.13.0rc2)\n",
      "Requirement already satisfied: absl-py in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mediapipe==0.10.9) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mediapipe==0.10.9) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mediapipe==0.10.9) (23.5.26)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mediapipe==0.10.9) (1.23.1)\n",
      "Requirement already satisfied: opencv-contrib-python in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mediapipe==0.10.9) (4.9.0.80)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mediapipe==0.10.9) (3.20.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mediapipe==0.10.9) (0.4.6)\n",
      "Requirement already satisfied: tensorflow-macos==2.13.0-rc2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow==2.13.0rc2) (2.13.0rc2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (1.6.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (16.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (23.1)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (58.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (1.62.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0rc0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1rc0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (2.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (4.41.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sounddevice>=0.4.4->mediapipe==0.10.9) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (0.42.0)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe==0.10.9) (2.21)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (2.3.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (2022.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0-rc2->tensorflow==2.13.0rc2) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe==0.10.9 opencv-python matplotlib scikit-learn tensorflow==2.13.0rc2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4889e99b-89ef-4e73-960d-311dae319fcd",
   "metadata": {},
   "source": [
    "### Importing Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e13bfc1-c27b-4105-849c-fac0fe456377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np \n",
    "import os \n",
    "import time \n",
    "import mediapipe as mp \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1623da9-5a1e-4516-8bd9-7266702143c0",
   "metadata": {},
   "source": [
    "### Extracting Key Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d4d5bba-d6a1-4445-966a-a746dc5bf44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic                       # Holistic model \n",
    "mp_drawing = mp.solutions.drawing_utils                  # Drawing utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e98a76d-5348-4e9f-a38b-696ac8fe4097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(img, model): \n",
    "    \"\"\"\n",
    "        Processes an image using a MediaPipe model to detect holistic features.\n",
    "\n",
    "        Parameters:\n",
    "        - image: The input image in BGR format (as read by OpenCV).\n",
    "        - model: A MediaPipe model instance configured for holistic detection (e.g., mp_holistic.Holistic).\n",
    "\n",
    "        Returns:\n",
    "        - image_rgb: The input image converted back from RGB to BGR format after processing.\n",
    "        - results: The detection results from the MediaPipe model, including landmarks for face, pose, and hands.\n",
    "    \"\"\"\n",
    "    image_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)              # COLOR CONVERSION BGR 2 RGB\n",
    "    results = model.process(image_rgb.copy())                       # Make prediction on a copy of the image\n",
    "    image_rgb = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)          # COLOR CONVERSION RGB 2 BGR \n",
    "    return image_rgb, results\n",
    "\n",
    "\n",
    "def annotate_with_landmarks(img, res): \n",
    "    \"\"\"\n",
    "        Draws landmarks and connections for face, pose, and hands on an image.\n",
    "\n",
    "        Parameters:\n",
    "        - img: The input image where landmarks will be drawn, expected in BGR format.\n",
    "        - res: The detection results containing landmarks detected by MediaPipe.\n",
    "               It includes face_landmarks, pose_landmarks, left_hand_landmarks, and right_hand_landmarks.\n",
    "\n",
    "        Returns:\n",
    "        - None. The function directly modifies the input image to draw landmarks and connections.\n",
    "        - These landmarks and connections are styled.\n",
    "    \"\"\"\n",
    "    # Face Landmarks\n",
    "    mp_drawing.draw_landmarks( \n",
    "        img, res.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "        mp_drawing.DrawingSpec(color=(247, 198, 246), thickness=1, circle_radius=1),  \n",
    "        mp_drawing.DrawingSpec(color=(115, 61, 191), thickness=1, circle_radius=1))        \n",
    "    # Pose connections\n",
    "    mp_drawing.draw_landmarks( \n",
    "        img, res.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "        mp_drawing.DrawingSpec(color=(250, 249, 187), thickness=2, circle_radius=4),  \n",
    "        mp_drawing.DrawingSpec(color=(158, 207, 255), thickness=2, circle_radius=2))           \n",
    "    # right hand connections \n",
    "    mp_drawing.draw_landmarks( \n",
    "        img, res.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "        mp_drawing.DrawingSpec(color=(224, 224, 164), thickness=2, circle_radius=4),  \n",
    "        mp_drawing.DrawingSpec(color=(167, 204, 169), thickness=2, circle_radius=2))      \n",
    "    # left hand connections \n",
    "    mp_drawing.draw_landmarks( \n",
    "        img, res.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "        mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),  \n",
    "        mp_drawing.DrawingSpec(color=(151, 199, 154), thickness=2, circle_radius=2))       \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "decef76e-96c7-4f0a-a3c1-a0d91b2215c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1711671358.808760       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "2024-03-28 17:15:59.200 Python[6489:12973154] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    }
   ],
   "source": [
    "webcam = cv2.VideoCapture(0)                               # device number 0\n",
    "with mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as holistic:\n",
    "    while webcam.isOpened():                               # while the webcam is  turned on\n",
    "        data = webcam.read()                               # get the data\n",
    "        if not data[0]:\n",
    "            break\n",
    "        # make landmarks for detection\n",
    "        img, res = mediapipe_detection(data[1], holistic)\n",
    "        # annoatate on video\n",
    "        annotate_with_landmarks(img, res)   \n",
    "        \n",
    "        cv2.imshow('Camera Feed', img)                     # display the image feed\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):             # close the feed using key \"q\"\n",
    "            break\n",
    "\n",
    "    # once the webcam is closed, close the display window\n",
    "    webcam.release()                              \n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)  \n",
    "    time.sleep(1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b5d9ca-ac10-4f60-9db0-2a7a11cfa4ab",
   "metadata": {},
   "source": [
    "### Getting Essential Key Point Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a32bae7-fe55-4e7c-923b-b9a5fbcb5849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks(landmarks, dimensions, default_value=0):\n",
    "    \"\"\"Extract landmarks as a flattened array or return a default array if landmarks are None.\"\"\"\n",
    "    if landmarks:\n",
    "        # Extract x, y, z, and optionally visibility from each landmark, based on the specified dimensions\n",
    "        return np.array([[getattr(res, dim) for dim in dimensions] for res in landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        # Return a flattened array of zeros based on the number of landmarks and the dimensions specified\n",
    "        return np.zeros(len(landmarks.landmark) * len(dimensions)) if landmarks else np.zeros(default_value)\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    # Define the dimensions to extract for each type of landmarks\n",
    "    pose_dimensions = ['x', 'y', 'z', 'visibility']\n",
    "    hand_and_face_dimensions = ['x', 'y', 'z']\n",
    "\n",
    "    # Extract keypoints for pose, face, and hands\n",
    "    pose = extract_landmarks(results.pose_landmarks, pose_dimensions, 33*4)\n",
    "    face = extract_landmarks(results.face_landmarks, hand_and_face_dimensions, 468*3)\n",
    "    left_hand = extract_landmarks(results.left_hand_landmarks, hand_and_face_dimensions, 21*3)\n",
    "    right_hand = extract_landmarks(results.right_hand_landmarks, hand_and_face_dimensions, 21*3)\n",
    "\n",
    "    # Concatenate all keypoints into a single array\n",
    "    keypoints = np.concatenate([pose, face, left_hand, right_hand])\n",
    "    \n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c965d49a-d6da-41ce-a91d-34a78e95625b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1662,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_keypoints(res).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be9bf62-dd57-4ac2-92cf-ac56124e0813",
   "metadata": {},
   "source": [
    "### Saving the collected Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cffaf0f-d603-4a3c-8907-9b0b3c582469",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('media-pipe-data')\n",
    "# Actions we would want to do on the TV\n",
    "actions = np.array(['on', 'off', 'vol_up', 'vol_down', 'netflix', 'amazon_prime'])\n",
    "# Number of sequences \n",
    "num_sequences = 30\n",
    "# Length of sequence\n",
    "seq_length = 30\n",
    "\n",
    "\n",
    "for action in actions:\n",
    "    for sequence in range(num_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except: \n",
    "            pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc24222-2767-4034-a7dc-b7adadc814d4",
   "metadata": {},
   "source": [
    "### Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d3fcdeb-5b89-4833-bc6f-3d0b098015b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1711671829.228980       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(image, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCollecting frames for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurr_action\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Video Number \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, (\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m12\u001b[39m), \n\u001b[1;32m     60\u001b[0m             cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.5\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mLINE_AA)\n\u001b[1;32m     61\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCamera Feed\u001b[39m\u001b[38;5;124m'\u001b[39m, image) \n\u001b[0;32m---> 62\u001b[0m     \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait 2 seconds on the first frame of each sequence\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:            \n\u001b[1;32m     64\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(image, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCollecting frames for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurr_action\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Video Number \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, (\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m12\u001b[39m), \n\u001b[1;32m     65\u001b[0m             cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.5\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mLINE_AA)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "webcam = cv2.VideoCapture(0) \n",
    "with mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "    for curr_action in actions:\n",
    "        for seq in range(num_sequences):\n",
    "            for f in range(seq_length): \n",
    "                data = webcam.read()\n",
    "                if not data[0]:\n",
    "                    break\n",
    "                image, results = mediapipe_detection(data[1], holistic)\n",
    "                annotate_with_landmarks(image, results)\n",
    "                \n",
    "                if f == 0:\n",
    "                    cv2.putText(image, 'STARTING DATA COLLECTION', (120, 200), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, f'Collecting frames for {curr_action} Video Number {seq}', (15, 12), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('Camera Feed', image) \n",
    "                    cv2.waitKey(2000)  # Wait 2 seconds on the first frame of each sequence\n",
    "                else:            \n",
    "                    cv2.putText(image, f'Collecting frames for {curr_action} Video Number {seq}', (15, 12), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('Camera Feed', image)  # Ensure consistent window name\n",
    "\n",
    "                keypoints = extract_keypoints(results)\n",
    "                np_path = os.path.join(DATA_PATH, curr_action, str(seq), str(f))\n",
    "                # cv2.waitKey(2000)  # Removed to avoid pausing every frame\n",
    "                \n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):  # Allow quick exit with 'q'\n",
    "                    break\n",
    "\n",
    "    webcam.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b769da42-45fb-4af0-8f08-669859d065d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "webcam.release()                              \n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)  \n",
    "time.sleep(1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29f0a28-e4ae-489d-ad0a-3a631611f96c",
   "metadata": {},
   "source": [
    "### Preprocessing Data and Labelling Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc81ad-e160-49d1-a2a5-c1f25603df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585f5ba7-5cf7-4a55-9171-107a0ec5d769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
