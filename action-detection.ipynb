{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e966c67b-eecd-46a0-bc97-24a7df955010",
   "metadata": {},
   "source": [
    "### Key Points "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4889e99b-89ef-4e73-960d-311dae319fcd",
   "metadata": {},
   "source": [
    "### Importing Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e13bfc1-c27b-4105-849c-fac0fe456377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np \n",
    "import os \n",
    "import time \n",
    "import mediapipe as mp \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1623da9-5a1e-4516-8bd9-7266702143c0",
   "metadata": {},
   "source": [
    "### Extracting Key Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d4d5bba-d6a1-4445-966a-a746dc5bf44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic                       # Holistic model \n",
    "mp_drawing = mp.solutions.drawing_utils                  # Drawing utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e98a76d-5348-4e9f-a38b-696ac8fe4097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(img, model): \n",
    "    \"\"\"\n",
    "        Processes an image using a MediaPipe model to detect holistic features.\n",
    "\n",
    "        Parameters:\n",
    "        - image: The input image in BGR format (as read by OpenCV).\n",
    "        - model: A MediaPipe model instance configured for holistic detection (e.g., mp_holistic.Holistic).\n",
    "\n",
    "        Returns:\n",
    "        - image_rgb: The input image converted back from RGB to BGR format after processing.\n",
    "        - results: The detection results from the MediaPipe model, including landmarks for face, pose, and hands.\n",
    "    \"\"\"\n",
    "    image_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)              # COLOR CONVERSION BGR 2 RGB\n",
    "    results = model.process(image_rgb.copy())                       # Make prediction on a copy of the image\n",
    "    image_rgb = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)          # COLOR CONVERSION RGB 2 BGR \n",
    "    return image_rgb, results\n",
    "\n",
    "\n",
    "def annotate_with_landmarks(img, res): \n",
    "    \"\"\"\n",
    "        Draws landmarks and connections for face, pose, and hands on an image.\n",
    "\n",
    "        Parameters:\n",
    "        - img: The input image where landmarks will be drawn, expected in BGR format.\n",
    "        - res: The detection results containing landmarks detected by MediaPipe.\n",
    "               It includes face_landmarks, pose_landmarks, left_hand_landmarks, and right_hand_landmarks.\n",
    "\n",
    "        Returns:\n",
    "        - None. The function directly modifies the input image to draw landmarks and connections.\n",
    "        - These landmarks and connections are styled.\n",
    "    \"\"\"\n",
    "    # Face Landmarks\n",
    "    mp_drawing.draw_landmarks( \n",
    "        img, res.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "        mp_drawing.DrawingSpec(color=(247, 198, 246), thickness=1, circle_radius=1),  \n",
    "        mp_drawing.DrawingSpec(color=(115, 61, 191), thickness=1, circle_radius=1))        \n",
    "    # Pose connections\n",
    "    mp_drawing.draw_landmarks( \n",
    "        img, res.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "        mp_drawing.DrawingSpec(color=(250, 249, 187), thickness=2, circle_radius=4),  \n",
    "        mp_drawing.DrawingSpec(color=(158, 207, 255), thickness=2, circle_radius=2))           \n",
    "    # right hand connections \n",
    "    mp_drawing.draw_landmarks( \n",
    "        img, res.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "        mp_drawing.DrawingSpec(color=(224, 224, 164), thickness=2, circle_radius=4),  \n",
    "        mp_drawing.DrawingSpec(color=(167, 204, 169), thickness=2, circle_radius=2))      \n",
    "    # left hand connections \n",
    "    mp_drawing.draw_landmarks( \n",
    "        img, res.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "        mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),  \n",
    "        mp_drawing.DrawingSpec(color=(151, 199, 154), thickness=2, circle_radius=2))       \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "decef76e-96c7-4f0a-a3c1-a0d91b2215c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1711671358.808760       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "2024-03-28 17:15:59.200 Python[6489:12973154] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    }
   ],
   "source": [
    "webcam = cv2.VideoCapture(0)                               # device number 0\n",
    "with mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as holistic:\n",
    "    while webcam.isOpened():                               # while the webcam is  turned on\n",
    "        data = webcam.read()                               # get the data\n",
    "        if not data[0]:\n",
    "            break\n",
    "        # make landmarks for detection\n",
    "        img, res = mediapipe_detection(data[1], holistic)\n",
    "        # annoatate on video\n",
    "        annotate_with_landmarks(img, res)   \n",
    "        \n",
    "        cv2.imshow('Camera Feed', img)                     # display the image feed\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):             # close the feed using key \"q\"\n",
    "            break\n",
    "\n",
    "    # once the webcam is closed, close the display window\n",
    "    webcam.release()                              \n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)  \n",
    "    time.sleep(1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b5d9ca-ac10-4f60-9db0-2a7a11cfa4ab",
   "metadata": {},
   "source": [
    "### Getting Essential Key Point Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a32bae7-fe55-4e7c-923b-b9a5fbcb5849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks(landmarks, dimensions, default_value=0):\n",
    "    \"\"\"Extract landmarks as a flattened array or return a default array if landmarks are None.\"\"\"\n",
    "    if landmarks:\n",
    "        # Extract x, y, z, and optionally visibility from each landmark, based on the specified dimensions\n",
    "        return np.array([[getattr(res, dim) for dim in dimensions] for res in landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        # Return a flattened array of zeros based on the number of landmarks and the dimensions specified\n",
    "        return np.zeros(len(landmarks.landmark) * len(dimensions)) if landmarks else np.zeros(default_value)\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    # Define the dimensions to extract for each type of landmarks\n",
    "    pose_dimensions = ['x', 'y', 'z', 'visibility']\n",
    "    hand_and_face_dimensions = ['x', 'y', 'z']\n",
    "\n",
    "    # Extract keypoints for pose, face, and hands\n",
    "    pose = extract_landmarks(results.pose_landmarks, pose_dimensions, 33*4)\n",
    "    face = extract_landmarks(results.face_landmarks, hand_and_face_dimensions, 468*3)\n",
    "    left_hand = extract_landmarks(results.left_hand_landmarks, hand_and_face_dimensions, 21*3)\n",
    "    right_hand = extract_landmarks(results.right_hand_landmarks, hand_and_face_dimensions, 21*3)\n",
    "\n",
    "    # Concatenate all keypoints into a single array\n",
    "    keypoints = np.concatenate([pose, face, left_hand, right_hand])\n",
    "    \n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c965d49a-d6da-41ce-a91d-34a78e95625b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1662,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_keypoints(res).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be9bf62-dd57-4ac2-92cf-ac56124e0813",
   "metadata": {},
   "source": [
    "### Saving the collected Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cffaf0f-d603-4a3c-8907-9b0b3c582469",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('media-pipe-data')\n",
    "# Actions we would want to do on the TV\n",
    "actions = np.array(['on', 'off', 'vol_up', 'vol_down', 'netflix', 'amazon_prime'])\n",
    "# Number of sequences \n",
    "num_sequences = 30\n",
    "# Length of sequence\n",
    "seq_length = 30\n",
    "\n",
    "\n",
    "for action in actions:\n",
    "    for sequence in range(num_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except: \n",
    "            pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc24222-2767-4034-a7dc-b7adadc814d4",
   "metadata": {},
   "source": [
    "### Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d3fcdeb-5b89-4833-bc6f-3d0b098015b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1711671829.228980       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(image, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCollecting frames for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurr_action\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Video Number \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, (\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m12\u001b[39m), \n\u001b[1;32m     60\u001b[0m             cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.5\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mLINE_AA)\n\u001b[1;32m     61\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCamera Feed\u001b[39m\u001b[38;5;124m'\u001b[39m, image) \n\u001b[0;32m---> 62\u001b[0m     \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait 2 seconds on the first frame of each sequence\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:            \n\u001b[1;32m     64\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(image, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCollecting frames for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurr_action\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Video Number \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, (\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m12\u001b[39m), \n\u001b[1;32m     65\u001b[0m             cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.5\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mLINE_AA)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "webcam = cv2.VideoCapture(0) \n",
    "with mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "    for curr_action in actions:\n",
    "        for seq in range(num_sequences):\n",
    "            for f in range(seq_length): \n",
    "                data = webcam.read()\n",
    "                if not data[0]:\n",
    "                    break\n",
    "                image, results = mediapipe_detection(data[1], holistic)\n",
    "                annotate_with_landmarks(image, results)\n",
    "                \n",
    "                if f == 0:\n",
    "                    cv2.putText(image, 'STARTING DATA COLLECTION', (120, 200), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, f'Collecting frames for {curr_action} Video Number {seq}', (15, 12), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('Camera Feed', image) \n",
    "                    cv2.waitKey(2000)  # Wait 2 seconds on the first frame of each sequence\n",
    "                else:            \n",
    "                    cv2.putText(image, f'Collecting frames for {curr_action} Video Number {seq}', (15, 12), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('Camera Feed', image)  # Ensure consistent window name\n",
    "\n",
    "                keypoints = extract_keypoints(results)\n",
    "                np_path = os.path.join(DATA_PATH, curr_action, str(seq), str(f))\n",
    "                # cv2.waitKey(2000)  # Removed to avoid pausing every frame\n",
    "                \n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):  # Allow quick exit with 'q'\n",
    "                    break\n",
    "\n",
    "    webcam.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b769da42-45fb-4af0-8f08-669859d065d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "webcam.release()                              \n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)  \n",
    "time.sleep(1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29f0a28-e4ae-489d-ad0a-3a631611f96c",
   "metadata": {},
   "source": [
    "### Preprocessing Data and Labelling Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc81ad-e160-49d1-a2a5-c1f25603df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585f5ba7-5cf7-4a55-9171-107a0ec5d769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
